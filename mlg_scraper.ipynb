{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a9dd6efc-bc60-4a92-94f9-7aafd193862b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "\n",
    "def extract_channel_name(row):\n",
    "    try:\n",
    "        channel_name = row.find_element('tag name', 'td').text\n",
    "        return channel_name.split('@')[-1].strip()  \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting channel name: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def parse_table_rows(table_rows):\n",
    "    channels = []\n",
    "    for row in table_rows:\n",
    "        cells = row.find_all('td')\n",
    "        if cells and len(cells) > 1:  # Ensure row contains more than one cell\n",
    "            channel = {}\n",
    "            channel['rank'] = cells[0].get_text(strip=True).split('@')[0]\n",
    "            channel['channel_id'] = cells[0].get_text(strip=True).split('@')[1] if '@' in cells[0].get_text(strip=True) else None            \n",
    "            channel['description'] = cells[1].get_text(strip=True)  # Extract channel description\n",
    "            channels.append(channel)\n",
    "    return channels\n",
    "    \n",
    "def scrape_top30_channels(link):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  \n",
    "    options.add_argument(\"--window-size=1920x1080\")\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path='chromedriver', options=options)\n",
    "    driver.get(link)\n",
    "\n",
    "    try:\n",
    "        cookie_element = driver.find_element(By.XPATH, '//div[@class=\"bottom__cookie-block\"]')\n",
    "        ok_button = cookie_element.find_element(By.CLASS_NAME, 'ok')\n",
    "        actions = ActionChains(driver)\n",
    "        actions.move_to_element(cookie_element).click(ok_button).perform()\n",
    "    except Exception as e:\n",
    "        print(f\"Error accepting cookies for {link}: {str(e)}\")\n",
    "\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.invisibility_of_element_located((By.CLASS_NAME, \"bottom__cookie-block\")))\n",
    "    except TimeoutException:\n",
    "        print(f\"Timeout: Unable to make the element invisible for {link}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        show_all_element = driver.find_element(By.XPATH, '//div[@class=\"more_rows pk\"]/a')\n",
    "        driver.execute_script(\"arguments[0].click();\", show_all_element)\n",
    "    except NoSuchElementException:\n",
    "        show_all_element = driver.find_element(By.XPATH, '//div[@class=\"more_rows\"]/a')\n",
    "        driver.execute_script(\"arguments[0].click();\", show_all_element)\n",
    "    #except NoSuchElementException:\n",
    "        #print(f\"'Все позиции' link not found for {link}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking 'Все позиции' link for {link}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "    # At this point, all the rows are visible. Get the table and parse the rows.\n",
    "    table = driver.find_element(By.XPATH, '//div[@class=\"block_tables\"]//table')\n",
    "    rows = table.find_elements(By.CSS_SELECTOR, 'tbody > tr:not(.rcount-row)')\n",
    "\n",
    "    # Create BeautifulSoup objects from the row's HTML and parse them\n",
    "    top_channels = parse_table_rows([BeautifulSoup(row.get_attribute('outerHTML'), 'html.parser') for row in rows])\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return top_channels  # Now this function returns a list of dicts containing channel information\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('top30_links.csv')\n",
    "    df['year'] = df['year'].astype(int)\n",
    "\n",
    "    if os.path.exists('top30_channels_data.pkl'):\n",
    "        with open('top30_channels_data.pkl', 'rb') as file:\n",
    "            top30_channels_by_year = pickle.load(file)\n",
    "    else:\n",
    "        top30_channels_by_year = {}\n",
    "\n",
    "    channel_descriptions = {}  # Add this line to store the most recent descriptions\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        year = row['year']\n",
    "        link = row['link']\n",
    "        month = row['month']\n",
    "\n",
    "        if year not in top30_channels_by_year:\n",
    "            top30_channels_by_year[year] = []\n",
    "\n",
    "        print(f\"Scraping data for year: {year}, month: {month}\")\n",
    "\n",
    "        channel_info = scrape_top30_channels(link)\n",
    "        top30_channels_by_year[year].extend(channel_info)\n",
    "\n",
    "        # Update the channel_descriptions dictionary with the most recent descriptions\n",
    "        for channel in channel_info:\n",
    "            channel_descriptions[channel['channel_id']] = channel['description']\n",
    "\n",
    "        with open('top30_channels_data.pkl', 'wb') as file:\n",
    "            pickle.dump(top30_channels_by_year, file)\n",
    "\n",
    "    common_channels_by_year = {\n",
    "        year: set(channel['channel_id'] for channel in channels) \n",
    "        for year, channels in top30_channels_by_year.items()\n",
    "    }\n",
    "\n",
    "    channels = set.union(*common_channels_by_year.values())\n",
    "    data = {\n",
    "        channel: [channel in common_channels_by_year.get(year, set()) for year in range(2017, 2024)] \n",
    "        for channel in channels\n",
    "    }\n",
    "    common_channels_df = pd.DataFrame(data, index=range(2017, 2024)).T\n",
    "\n",
    "    # Add a column to the DataFrame with the most recent description for each channel\n",
    "    common_channels_df['description'] = [channel_descriptions.get(channel_id, '') for channel_id in common_channels_df.index]\n",
    "\n",
    "    common_channels_df.columns = ['2017', '2018', '2019', '2020', '2021', '2022', '2023', 'description']\n",
    "    common_channels_df.index.name = 'channel name'\n",
    "\n",
    "    common_channels_df.to_csv('common_channels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "771f8d3a-4d09-4e7b-a84a-035978db7447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for year: 2017, month: November\n",
      "Scraping data for year: 2018, month: February\n",
      "Scraping data for year: 2018, month: July\n",
      "Scraping data for year: 2019, month: January\n",
      "Scraping data for year: 2019, month: July\n",
      "Scraping data for year: 2020, month: January\n",
      "Scraping data for year: 2020, month: July\n",
      "Scraping data for year: 2021, month: January\n",
      "Scraping data for year: 2021, month: February\n",
      "Scraping data for year: 2021, month: July\n",
      "Scraping data for year: 2022, month: January\n",
      "Scraping data for year: 2022, month: July\n",
      "Scraping data for year: 2023, month: January\n",
      "Scraping data for year: 2023, month: June\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
